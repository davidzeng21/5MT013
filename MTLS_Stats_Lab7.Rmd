---
title: "R Notebook"
output: html_notebook
---

```{r}
library(dplyr)
library(tibble)
library(ggplot2)
library(broom)
library(skimr)
library(ggfortify)
```

```{r}
# load the data
load("E:/Xuexi/5MT013/5MT013/BeatAML2_clean.RData")
```


```{r}
setwd("E:/Xuexi/5MT013/5MT013")
```

```{r}
# check the data
JOSD1<-rnaNorm['ENSG00000100221',]
SCYL3<-rnaNorm['ENSG00000000457',]
data<-data.frame(JOSD1, SCYL3)
```

```{r}
sapply(data, function(x) c(Length = length(x),
                                        Minimum = min(x, na.rm = TRUE),
                                        Maximum = max(x, na.rm = TRUE),
                                        Mean = mean(x, na.rm = TRUE),
                                        Median = median(x, na.rm = TRUE),
                                        SD = sd(x, na.rm = TRUE)))
```

```{r}
skim(data)
```

### 2. Produce a scatter plot to visually assess the association between the two variables.

```{r}
# plot the data
library(ggplot2)
ggplot(data) +
  aes(x =JOSD1, y = SCYL3) +
  geom_point()

```

d)  Use R to obtain Pearson correlation coeffecient.

    ```{r}
    cor(data$JOSD1, data$SCYL3)
    ```

<!-- -->

### 3. Fit a simple linear regression model with JOSD1 as the outcome and SCYL3 as the exposure.

```{r}
# correlation
mod1<-lm(JOSD1 ~ SCYL3, data = data)
summary(mod1)


```

```{r}
coef(summary(mod1))
confint(mod1, level=0.9)
confint(mod1, level=0.95)
confint(mod1, level=0.99)
```

f)  Use R to check your answers for (d) and (e), and to obtain 95% confidence intervals for these estimates.

```{r}
predict(mod1, newdata = data.frame(SCYL3 = c(0, 3.96)), interval = "prediction")
predict(mod1, newdata = data.frame(SCYL3 = c(0, 3.96)), interval = "confidence")
```

g)  Create a new variable which contains the residuals from this regression model. Draw a scatterplot of residuals (y-axis) against SCYL3 (the predictor, x-axis). Do the model assumptions hold?

```{r}
modfit <- augment(mod1, data)
ggplot(modfit) + geom_point(aes(SCYL3, .resid))
```

```{r}
# or use this to get R's set of standard lm diagnostic plots
autoplot(mod1) # from ggfortify
```

```{r}
# regression
ggplot(data) +
  aes(x = SCYL3, y = JOSD1) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

```

### 4. Create a new variable which is a centered version of SCYL3, centered at 4 (which is approximately the mean and the median). Fit a linear regression model with this as the (only) predictor in the model.

Compare the output from this model to the model you fitted in (3). What has changed, and what has remained the same? Which of your answers from (3) will change?

```{r}
data <- data |> 
    mutate(cSCYL3 = SCYL3 - 4)
mod2 <- lm(JOSD1 ~ cSCYL3, data = data)

summary(mod2)
coef(summary(mod2))
```

The coeffecient for SCYL3 is unchanged. Centering a predictor variable simply shifts it along the x-axis, and so does not change the slope of the regression line. The Standard Error is also unchanged. 

The value of the intercept has changed; this is now the expected value of a person with SCYL3=4 (close to the mean), so we would expect the predicted value to be close to the mean of JOSD1: it is (6.94, compared to the mean value of 6.98). 
Note that the Standard Error is smaller (it’s now 0.030, was 0.21). This is because we are estimating the intercept at a point in the middle of the data, where we can be more certain where it will be; at the ends of the regression line, far from the data, a small change in slope will produce a large change in intercept, so we have less certainty about the estimate. 

The value of $R^2$ is the same, as the predictor variable still explains the same proportion of variance in the outcome as before the transformation. A linear transformation of a predictor variable will not change $R^2$ (but a non-linear transformation will).

The value of the F-statistic is the same, as this is a test of the overall model fit, and the model fit is the same.

### 5. Write a short paragraph to summarise the association between the two variables.
There is a negative correlation between observed values of normalised SCYL3 and normalised JOSD1 in this dataset, with a Person correlation coefficient of $-0.56$. Using a linear regression model fitted to the data from 442 participants we found strong evidence ($p<0.001$) that a one unit increase in normalised SCYL3 was associated with a 0.76 unit decrease in normalised JOSD1, with a $95%$ confidence interval of $(-0.87, -0.66)$. The R2 value of 0.31 indicates that about one-third of the variability of JOSD1 in this dataset can be explained by SCYL3. There was no evidence that the assumptions of the linear regression model were not satisfied.

### 6. For this question we will use some simulated data. Import the data in `“Lab07_Q6_data.txt”` into R, and examine the contents. We will treat Y as the (continuous) outcome and X as a (continuous) predictor.

```{r}
simdata <- read.table("Lab7_Q6_data.txt", header = TRUE)
head(simdata)
```
a) Make a scatter plot of Y against X. Does it look like there is an
association between the two variables? Does the association look linear?

```{r}
ggplot(simdata) + geom_point(aes(X, Y))
```
 b) Fit a simple linear regression model of Y on X. What is the estimate for
the effect of X on Y? Is there strong evidence for a true association?
```{r}
mod3 <- lm(Y ~ X, data = simdata)
summary(mod3)
```
```{r}
# regression
ggplot(simdata) +
  aes(x = X, y = Y) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

```
c) Examine the residuals from this model. What conclusions do you draw about the assumptions required for linear regression?

```{r}
autoplot(mod3)
```
# d) Create a new variable which is the natural log of Y. Make a scatter plot of this new variable against X. Does it look like there is an association between these two variables? Is it a linear association?

```{r}
simdata <- simdata |>
    mutate(lgY = log(Y))

ggplot(simdata) + geom_point(aes(X, lgY))
```

```{r}
# fit a linear regression model
mod4 <- lm(lgY ~ X, data = simdata)
summary(mod4)
```
```{r}
autoplot(mod4)
```

```{r}
# regression
confint(mod4)
ggplot(simdata) +
  aes(x = X, y = lgY) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

f) To interpret the coefficient for X, we have to think about the impact a one unit increase in X will have on Y (on the natural scale).








